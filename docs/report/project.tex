\documentclass{article}
\usepackage{minted}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage[hyperref]{project}
\usepackage{pythonhighlight}
\usepackage{booktabs,chemformula}
\renewcommand{\UrlFont}{\ttfamily\small}

\aclfinalcopy


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Spring 2023 CS598 DL4H: Graph Attention Networks Reproducibility Project}

\author
{Valle-Mena, Ricardo Ruy and Soni, Kushagra \\
    \texttt{\{rrv4, soni14\}@illinois.edu}
    \\[2em]
    Group ID: 179\\
    Paper ID: 7\\
    Presentation link: \url{https://youtu.be/OLJBoOzjBHg} \\
    Code link: \url{https://github.com/kushagrasoni/CS598_DLH_GAT_Implementation}
    \\[2em]
}

\begin{document}
    \maketitle
\section{Introduction}\label{sec:introduction}
    Graph Attention Networks (GATs)~\cite{velickovic2018graph} are a type of neural network architecture designed for
    processing graph-structured data.
    They aim to overcome the limitations of previous architectures by being able to operate on arbitrarily structured graphs in a parallelizable manner.
    GATs use a shared masked self-attention mechanism to assign weights to each node's neighbors and combine their features, resulting in a new set of features for the node in question.
    GATs have shown promising results in various graph-based tasks, including node classification and link prediction.

    \section{Scope of Reproducibility}\label{sec:scope-of-reproducibility}

    \subsection{Background and Problem Statement}\label{sec:background-and-problem-statement}
    \input{background-and-problem-statement}

    \subsection{Objectives}\label{sec:objectives}
    Our goal was to reproduce the results reported in the original paper, which used the Cora, Citeseer, Pubmed, and Protein-protein interaction datasets.
    Specifically, we aimed for:
    \begin{itemize}
        \item Classification accuracies of approximately 83.0\%, 72.5\%, and 79\% in the Cora, Citeseer, and Pubmed
        datasets, respectively,
        \item Micro-averaged F1 score of approximately 0.973 in the protein-protein interaction dataset.
    \end{itemize}

    Furthermore, we trained two one-layer GAT models and a three-layer GAT
    model on the Cora, Citeseer and Pubmed datasets as ablation studies.  We
    hypothesized that one-layer models would perform worse than two-layer
    models, which would in turn perform worse than three-layer models.

    \section{Methodology}\label{sec:methodology}
    We tried a total of 5 GAT implementations, four of which were our own, and
    one of which is from the Pytorch Geometric library. One of our
    implementations was strongly influenced by the paper's authors'
    implementation ~\cite{petarvgatgithub}.

    All datasets from the paper are publicly available in multiple locations,
    including the Pytorch Geometric library. We have been running our
    experiments locally. Despite the datasets being relatively small, the PPI
    dataset turned out to be sufficiently large to take a prohibitively long
    time to run. We therefore ended up not having results to present for the
    PPI data.

    \subsection{Model Description}\label{subsec:model-description}
    \input{model-description}

    \subsection{Data Description}\label{subsec:data-description}
    \input{data-description}

    \subsection{Hyperparameters}\label{subsec:hyperparameters}
    There aren't really any hyperparameters to speak of, other than those
    described in the model description section (the number of layers, L2
    regularization coefficient, etc.). All hyperparameters were therfore taken
    directly from the paper.

    \subsection{Model Implementation}\label{subsec:model-implementation}
    \input{model-implementation}

    \subsection{Computational Requirements}\label{subsec:computational-requirements}
    \input{computational-requirements}

    \section{Results}\label{sec:results}
    \input{results}

    \section{Discussion}\label{sec:discussion}
    \input{discussion}

    \section{Communication with original authors}
    We did not communicate with the paper's authors at all.

    \bibliographystyle{unsrt} %Reference style.
    \bibliography{references}

\end{document}

