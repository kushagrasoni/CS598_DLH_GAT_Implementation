\documentclass{article}
\usepackage{minted}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage[hyperref]{project}
\usepackage{pythonhighlight}
\usepackage{booktabs,chemformula}
\renewcommand{\UrlFont}{\ttfamily\small}

\aclfinalcopy


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Spring 2023 CS598 DL4H: Graph Attention Networks Reproducibility Project}

\author
{Valle-Mena, Ricardo Ruy and Soni, Kushagra \\
    \texttt{\{rrv4, soni14\}@illinois.edu}
    \\[2em]
    Group ID: 179\\
    Paper ID: 7\\
    Code link: \url{https://github.com/kushagrasoni/CS598_DLH_GAT_Implementation}
    \\[2em]
}

\begin{document}
    \maketitle
\section{Introduction}\label{sec:introduction}
    Graph Attention Networks (GATs)~\cite{velickovic2018graph} are a type of neural network architecture designed for
    processing graph-structured data.
    They aim to overcome the limitations of previous architectures by being able to operate on arbitrarily structured graphs in a parallelizable manner.
    GATs use a shared masked self-attention mechanism to assign weights to each node's neighbors and combine their features, resulting in a new set of features for the node in question.
    GATs have shown promising results in various graph-based tasks, including node classification and link prediction.

    \section{Scope of Reproducibility}\label{sec:scope-of-reproducibility}

    \subsection{Background and Problem Statement}\label{sec:background-and-problem-statement}
    \input{background-and-problem-statement}

    \subsection{Objectives}\label{sec:objectives}
    Our goal was to reproduce the results reported in the original paper, which used the Cora, Citeseer, Pubmed, and Protein-protein interaction datasets. Specifically, we aimed for:
    \begin{itemize}
        \item Classification accuracies of approximately 83.0\%, 72.5\%, and 79\% in the Cora, Citeseer, and Pubmed
        datasets, respectively,
        \item Micro-averaged F1 score of approximately 0.973 in the protein-protein interaction dataset.
    \end{itemize}

    Furthermore, we conducted the following ablation studies:
    \begin{itemize}
        \item Using one, two, and three layer models for the Citeseer, Cora, and Pubmed datasets
	\item Not using dropout
	\item Not using L2 regularization
	\item Not using dropout or L2 regularization
    \end{itemize}
    We hypothesized that one-layer models would perform worse than two-layer models, which would in turn perform worse than three-layer models. We also hypothesized that not using dropout would perform worse than using dropout, that not using L2 regularization would perform worse than using L2 regularization, and that using neither dropout nor L2 regularization would perform worse of all.

    \section{Methodology}\label{sec:methodology}
    We tried a total of 5 GAT implementations, four of which were our own. First, we tried using the Pytorch Geometric implementation since we have used this library previously in this course. We then realized this implementation does not provide enough flexibility to accurately reproduce the paper's methodology, so we reimplemented GAT twice using more basic Pytorch Geometric primitives, specifically the GATConv and GATv2Conv classes. GATConv implements part of the original GAT model from our paper, and GATv2Conv is a modified implementation based on a subsequent paper (https://arxiv.org/abs/2105.14491) which claims to have made the attention mechanism used in GAT more powerful. We then tried implementing the GAT model using Pytorch primitives to get a deeper understanding of the model. We first tried following the paper's author's implementation (https://github.com/PetarV-/GAT), but we appear to have made a mistake. Our model improves up to roughly 50% accuracy on the training data using the Cora dataset and then stops improving. We then tried implementing the GAT model again based on the GATLayerImp2 (https://github.com/gordicaleksa/pytorch-GAT/blob/main/models/definitions/GAT.py), but again appear to have made a mistake that stops the model from improving past roughly 45% accuracy on the training data from the Cora dataset.

    All datasets from the paper are publicly available in multiple locations, including the Pytorch Geometric library. We have been running our experiments locally. Despite the datasets being relatively small, the PPI dataset turned out to be sufficiently large to take a prohibitively long time to run. We therefore do not provide the same amount of results for the PPI dataset as for the other three.

    \subsection{Model Description}\label{subsec:model-description}
    \input{model-description}

    \subsection{Data Description}\label{subsec:data-description}
    \input{data-description}

    \subsection{Model Implementation}\label{subsec:model-implementation}
    \input{model-implementation}

    \subsection{Computational Requirements}\label{subsec:computational-requirements}
    \input{computational-requirements}

    \section{Results}\label{sec:results}
    \input{results}

    \section{Limitations and Challenges}\label{sec:limitations-and-challenges}
    As mentioned previously, the PPI dataset turned out to be sufficiently large to make experimenting with it prohibitively slow, so we were unable to experiment with it as thoroughly as with the other three datasets. We are unsure whether getting our code to run on a GPU would change this.

    \bibliographystyle{unsrt} %Reference style.
    \bibliography{references}

\end{document}

