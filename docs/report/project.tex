\documentclass{article}
\usepackage{minted}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage[hyperref]{project}
\usepackage{pythonhighlight}
\usepackage{booktabs,chemformula}
\renewcommand{\UrlFont}{\ttfamily\small}

\aclfinalcopy


\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Spring 2023 CS598 DL4H: Graph Attention Networks Reproducibility Project}

\author
{Valle-Mena, Ricardo Ruy and Soni, Kushagra \\
    \texttt{\{rrv4, soni14\}@illinois.edu}
    \\[2em]
    Group ID: 179\\
    Paper ID: 7\\
    Code link: \url{https://github.com/kushagrasoni/CS598_DLH_GAT_Implementation}
    \\[2em]
}

\begin{document}
    \maketitle
\section{Introduction}\label{sec:introduction}
    Graph Attention Networks (GATs)~\cite{velickovic2018graph} are a type of neural network architecture designed for
    processing graph-structured data.
    They aim to overcome the limitations of previous architectures by being able to operate on arbitrarily structured graphs in a parallelizable manner.
    GATs use a shared masked self-attention mechanism to assign weights to each node's neighbors and combine their features, resulting in a new set of features for the node in question.
    GATs have shown promising results in various graph-based tasks, including node classification and link prediction.

    \section{Scope of Reproducibility}\label{sec:scope-of-reproducibility}

    \subsection{Background and Problem Statement}\label{sec:background-and-problem-statement}
    \input{background-and-problem-statement}

    \subsection{Objectives}\label{sec:objectives}
    Our goal was to reproduce the results reported in the original paper, which used the Cora, Citeseer, Pubmed, and Protein-protein interaction datasets.
    Specifically, we aimed for:
    \begin{itemize}
        \item Classification accuracies of approximately 83.0\%, 72.5\%, and 79\% in the Cora, Citeseer, and Pubmed
        datasets, respectively,
        \item Micro-averaged F1 score of approximately 0.973 in the protein-protein interaction dataset.
    \end{itemize}

    Furthermore, we conducted the following ablation studies:
    \begin{itemize}
        \item Using one, two, and three layer models for the Citeseer, Cora, and Pubmed datasets
	\item Not using dropout
	\item Not using L2 regularization
	\item Not using dropout or L2 regularization
    \end{itemize}
    We hypothesized that one-layer models would perform worse than two-layer models, which would in turn perform worse than three-layer models.
    We also hypothesized that not using dropout would perform worse than using dropout, that not using L2 regularization would perform worse than using L2 regularization, and that using neither dropout nor L2 regularization would perform worse of all.

    \section{Methodology}\label{sec:methodology}
    We tried a total of 5 GAT implementations, four of which were our own, and
    one of which is from the Pytorch Geometric library. One of our
    implementations was strongly influenced by the paper's authors'
    implementation ~\cite{petarvgatgithub}.

    All datasets from the paper are publicly available in multiple locations,
    including the Pytorch Geometric library. We have been running our
    experiments locally. Despite the datasets being relatively small, the PPI
    dataset turned out to be sufficiently large to take a prohibitively long
    time to run. We therefore do not provide the same amount of results for the
    PPI dataset as for the other three.

    \subsection{Model Description}\label{subsec:model-description}
    \input{model-description}

    \subsection{Data Description}\label{subsec:data-description}
    \input{data-description}

    \subsection{Hyperparameters}\label{subsec:hyperparameters}
    There aren't really any hyperparameters to speak of, other than those
    described in the model description section (the number of layers, L2
    regularization coefficient, etc.). All hyperparameters were therfore taken
    directly from the paper.

    \subsection{Model Implementation}\label{subsec:model-implementation}
    \input{model-implementation}

    \subsection{Computational Requirements}\label{subsec:computational-requirements}
    \input{computational-requirements}

    \section{Results}\label{sec:results}
    \input{results}

    \section{Discussion}\label{sec:discussion}
    \input{discussion}

    \section{Communication with original authors}
    We did not communicate with the paper's authors at all.

    \bibliographystyle{unsrt} %Reference style.
    \bibliography{references}

\end{document}

