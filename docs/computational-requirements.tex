To reproduce the results reported in the GAT paper, we implemented the GAT model using PyTorch version 1.9.0~\cite{paszke2019pytorch} on two platforms:
\begin{itemize}
    \item two local systems, a macbook and a Windows both with an Intel Core i7 CPU (2.6 GHz and 1.8 GHz), 16GB RAM,
    and Intell UHD Graphics
    card
    GTX 1080
    Ti GPU;
    \item Google Colab's free GPU instance with 12GB of RAM\@.
\end{itemize}

On the local system, we installed PyTorch and all necessary dependencies using the pip package manager.
At part of next stage of the project, we will try to use the NVIDIA CUDA Toolkit to enable GPU acceleration.
Training the GAT model on the Cora and Citeseer datasets took approximately 2--3 seconds per epoch, depending on the
batch size and learning rate used.

On Google Colab, we used the provided Jupyter notebook environment, which includes PyTorch and all necessary dependencies pre-installed.
We also used Google's free GPU instance with 12GB of RAM, which allowed us to train the GAT model on larger datasets such as Pubmed and PPI.
Training the GAT model on the Pubmed dataset took approximately TBU minutes per epoch, while training on the PPI
dataset took approximately TBU per epoch.

Overall, the computational requirements for reproducing the GAT results on a local system are moderate, but may require a GPU for faster training times.
Google Colab provides a convenient and free platform for reproducing the results, but training times may be longer due to the limited RAM and shared GPU resources.
