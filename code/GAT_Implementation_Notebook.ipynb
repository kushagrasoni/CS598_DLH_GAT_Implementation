{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a2eb33",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook attempts to gather and organize all our code in one place such that whoever is grading our team finds it easy to consult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid, PPI\n",
    "from torch_geometric.nn import GAT\n",
    "from datetime import datetime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import scipy\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our first important step was to figure out how to load the data and run it through the GAT model. We discovered that both the data and an implementation of the model are available in the Pytorch Geometric library. We ran it as follows:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for dataset in ['citeseer', 'cora', 'pubmed']:\n",
    "    start = datetime.now()\n",
    "    # Load the data\n",
    "    dataset = Planetoid(root=f'../data/{dataset}', name=dataset)\n",
    "    model = GAT(\n",
    "        in_channels=dataset.num_features,\n",
    "        out_channels=dataset.num_classes,\n",
    "        hidden_channels=8,\n",
    "        num_layers=2,\n",
    "        heads=8,\n",
    "        dropout=0.6,\n",
    "        act='elu',\n",
    "        act_first=True\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(dataset.x, dataset.edge_index)\n",
    "        loss = F.cross_entropy(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    out = model(dataset.x, dataset.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = pred[dataset.test_mask].eq(dataset.y[dataset.test_mask]).sum().item() / int(dataset.test_mask.sum())\n",
    "    print('\\n\\n*****************************************************************************************************\\n')\n",
    "    print(f'                                         {dataset} ')\n",
    "    print(f'                                         Total Epochs: 200')\n",
    "    print(f'                                         Test Accuracy: {acc:.4f}')\n",
    "    print(f'                                         Time Taken: {datetime.now() - start}')\n",
    "    print('\\n*****************************************************************************************************\\n\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "d2e86bc1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We reused variants of the above code several times.\n",
    "\n",
    "Note that we should not have run the above model on the Pubmed dataset, because the model used on the Pubmed data in the paper is slightly different than the model used on the Cora and Citeseer datasets. We addressed this mistake later.\n",
    "\n",
    "We also ran the model on the PPI dataset as follows. We don't recommend actually running this because it is very slow and requires a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7133a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ppi_train = PPI('../data/ppi/')\n",
    "\n",
    "model = GAT(\n",
    "    in_channels=ppi_train.num_features,\n",
    "    out_channels=ppi_train.num_classes,\n",
    "    hidden_channels=256,\n",
    "    num_layers=3,\n",
    "    heads=4,\n",
    "    dropout=0.6,\n",
    "    act='elu',\n",
    "    act_first=True\n",
    ")\n",
    "\n",
    "# weight_decay applies L2 regularization on the model's parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "start = datetime.now()\n",
    "# Train model\n",
    "for epoch in range(2):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(ppi_train.x, ppi_train.edge_index)\n",
    "    loss = F.cross_entropy(out, ppi_train.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    pred = model(ppi_train.x, ppi_train.edge_index) > .5\n",
    "    f1 = sklearn.metrics.f1_score(ppi_train.y.detach().numpy(), pred.detach().numpy(), average='micro')\n",
    "    #print(f'Epoch {epoch + 1:03d}, Loss: {loss:.4f}, F1: {f1}')\n",
    "\n",
    "\n",
    "# Test the model\n",
    "ppi_test = PPI('../data/ppi/', 'test')\n",
    "model.eval()\n",
    "out = model(ppi_test.x, ppi_test.edge_index) > .5\n",
    "f1 = sklearn.metrics.f1_score(ppi_test.y.detach().numpy(), out.detach().numpy(), average='micro')\n",
    "print('\\n\\n*****************************************************************************************************\\n')\n",
    "print(f'                                         PPI Dataset ')\n",
    "print(f'                                         Total Epochs: 200')\n",
    "print(f'                                         F1 Score: {f1:.4f}')\n",
    "print(f'                                         Time Taken: {datetime.now() - start}')\n",
    "print('\\n*****************************************************************************************************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e593600",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At this point we realized that Pytorch Geometric's `GAT` class does not allow us to properly replicate the paper's methodology. Specifically, this implementation does not allow different layers within the model to have different numbers of attention heads or different activation functions.\n",
    "\n",
    "The next thing we did was make our own GAT implementation using Pytorch Geometric's `GATConv` class, which implements a single GAT layer. The following adheres to the architecture used in the paper on the Cora and Citeseer datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297f5cc7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GATCora(torch.nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch_geometric.nn.GATConv(heads=8, out_channels=8, in_channels=in_channels, dropout=.6)\n",
    "        self.act1 = torch.nn.ELU()\n",
    "        self.conv2 = torch_geometric.nn.GATConv(heads=1, out_channels=n_classes, in_channels=64, dopout=.6)\n",
    "        self.act2 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.act1(self.conv1(x, edge_index))\n",
    "        x = self.act2(self.conv2(x, edge_index))\n",
    "        return x\n",
    "    \n",
    "for dataset in ['citeseer', 'cora']:\n",
    "    start = datetime.now()\n",
    "    dataset = Planetoid(root=f'../data/{dataset}', name=dataset)\n",
    "    model = GATCora(dataset.num_features, dataset.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(dataset.x, dataset.edge_index)\n",
    "        loss = F.cross_entropy(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    out = model(dataset.x, dataset.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = pred[dataset.test_mask].eq(dataset.y[dataset.test_mask]).sum().item() / int(dataset.test_mask.sum())\n",
    "    print('\\n\\n*****************************************************************************************************\\n')\n",
    "    print(f'                                         {dataset} ')\n",
    "    print(f'                                         Total Epochs: 200')\n",
    "    print(f'                                         Test Accuracy: {acc:.4f}')\n",
    "    print(f'                                         Time Taken: {datetime.now() - start}')\n",
    "    print('\\n*****************************************************************************************************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463caef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similarly, the following is for the Pubmed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cdd68f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GATPubmed(torch.nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch_geometric.nn.GATConv(heads=8, out_channels=8, in_channels=in_channels, dropout=.6)\n",
    "        self.act1 = torch.nn.ELU()\n",
    "        self.conv2 = torch_geometric.nn.GATConv(heads=8, out_channels=n_classes, in_channels=64, dropout=.6, concat=False)\n",
    "        self.act2 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.act1(self.conv1(x, edge_index))\n",
    "        x = self.act2(self.conv2(x, edge_index))\n",
    "        return x\n",
    "    \n",
    "start = datetime.now()\n",
    "dataset = Planetoid(root=f'../data/pubmed', name='pubmed')\n",
    "model = GATPubmed(dataset.num_features, dataset.num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x, dataset.edge_index)\n",
    "    loss = F.cross_entropy(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "out = model(dataset.x, dataset.edge_index)\n",
    "pred = out.argmax(dim=1)\n",
    "acc = pred[dataset.test_mask].eq(dataset.y[dataset.test_mask]).sum().item() / int(dataset.test_mask.sum())\n",
    "print('\\n\\n*****************************************************************************************************\\n')\n",
    "print(f'                                         Pubmed ')\n",
    "print(f'                                         Total Epochs: 200')\n",
    "print(f'                                         Test Accuracy: {acc:.4f}')\n",
    "print(f'                                         Time Taken: {datetime.now() - start}')\n",
    "print('\\n*****************************************************************************************************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a66db17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We didn't make a new model for the PPI data because it took a very long time to run the first time and we wanted to play around with other stuff. Unfortunately, we never got the time to come back to the PPI model, and we discarded the results from our first and only run.\n",
    "\n",
    "We then realized the paper described an early stopping criterion, so we added it into our procedure as well. If we take Citeseer as an example, the training code would look as follows with the early stopping criterion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ed60c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up `optimizer`, `model` and `dataset` before this, as usual.\n",
    "best_epoch = 0\n",
    "best_loss = 1e10\n",
    "patience = 100\n",
    "best_acc = 0.0\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset.x, dataset.edge_index)\n",
    "    loss = F.cross_entropy(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    pred = model(dataset.x, dataset.edge_index).argmax(dim=1)\n",
    "    correct = int(pred[dataset.train_mask].eq(dataset.y[dataset.train_mask]).sum().item())\n",
    "    acc = correct / int(dataset.train_mask.sum())\n",
    "\n",
    "    if (acc >= best_acc) or (loss <= best_loss):\n",
    "        best_acc = np.max((acc, best_acc))\n",
    "        best_epoch = np.max((epoch, best_epoch))\n",
    "        best_loss = np.min((loss.detach().numpy(), best_loss))\n",
    "\n",
    "    if epoch - best_epoch > patience:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae2864",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We then decided to attempt to reimplement the GAT model using Pytorch instead of Pytorch Geometric. This would help us gain a deeper understanding of how the model works and would allow to do more interesting kinds of ablations studies, since it would allow us to control in detail what happens within each layer.\n",
    "\n",
    "Unfortunately, both our attempts to rewrite GAT from scratch failed. We appear to have made at least one mistake in each case, such that the models don't improve past roughly 45% to 50% accuracy on the training data from the Citeseer dataset, whereas we had seen our previous models reach accuracies well above 80% on training data.\n",
    "\n",
    "The following two cells are our two attempts at implementing GAT from scratch. The first is based on the paper's implementation (https://github.com/PetarV-/GAT) and the second is based on GATLayerImp2 (https://github.com/gordicaleksa/pytorch-GAT/blob/main/models/definitions/GAT.py#L349)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17b345",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GATHead(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias, in_drop=0.0, coef_drop=0.0, residual=False):\n",
    "        super().__init__()\n",
    "        self.bias1 = torch.nn.Parameter(bias)\n",
    "        self.bias2 = torch.nn.Parameter(torch.zeros(out_dim))\n",
    "        self.in_drop = in_drop\n",
    "        self.coef_drop = coef_drop\n",
    "        self.residual = residual\n",
    "\n",
    "        self.conv1 = torch.nn.Conv1d(in_dim, out_dim, 1, bias=False)\n",
    "        self.conv2 = torch.nn.Conv1d(out_dim, 1, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(in_dim, out_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if self.in_drop != 0.0:\n",
    "            data = torch.nn.functional.dropout(data, 1.0 - self.in_drop)\n",
    "\n",
    "        feats = self.conv1(data.permute(0, 2, 1))\n",
    "\n",
    "        f_1 = self.conv2(feats)\n",
    "        logits = f_1 + f_1.permute(0, 2, 1)\n",
    "        coefs = torch.nn.functional.softmax(torch.nn.functional.leaky_relu(logits) + self.bias1, dim=-1)\n",
    "\n",
    "        if self.coef_drop != 0.0:\n",
    "            coefs = torch.nn.functional.dropout(coefs, 1.0 - self.coef_drop)\n",
    "        if self.in_drop != 0.0:\n",
    "            feats = torch.nn.functional.dropout(feats, 1.0 - self.in_drop)\n",
    "\n",
    "        vals = torch.matmul(coefs.float(), feats.permute(0, 2, 1))\n",
    "        ret = vals + self.bias2\n",
    "\n",
    "        if self.residual:\n",
    "            if data.shape[-1] != ret.shape[-1]:\n",
    "                ret = ret + self.conv3(data)\n",
    "            else:\n",
    "                ret = ret + data\n",
    "\n",
    "        return ret\n",
    "\n",
    "class GATLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_heads, bias, in_drop, coef_drop, residual, concat):\n",
    "        super().__init__()\n",
    "        self.heads = torch.nn.ParameterList([GATHead(in_dim, out_dim, bias, in_drop, coef_drop, residual) for i in range(n_heads)])\n",
    "        self.concat = concat\n",
    "\n",
    "    def forward(self, data):\n",
    "        head_out = [self.heads[i](data) for i in range(len(self.heads))]\n",
    "        if self.concat:\n",
    "            return torch.concat(head_out, -1)\n",
    "        return torch.sum(torch.stack(head_out, -1), dim=-1) / len(head_out)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes, bias):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATLayer(n_features, 8, 8, bias, .6, .6, False, True)\n",
    "        self.conv2 = GATLayer(64, n_classes, 1, bias, .6, .6, False, False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.elu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return torch.nn.functional.softmax(x, dim=-1)\n",
    "\n",
    "# Taken directly from https://github.com/PetarV-/GAT/blob/master/utils/process.py\n",
    "def adj_to_bias(adj, sizes, nhood=1):\n",
    "    nb_graphs = adj.shape[0]\n",
    "    mt = np.empty(adj.shape)\n",
    "    for g in range(nb_graphs):\n",
    "        mt[g] = np.eye(adj.shape[1])\n",
    "        for _ in range(nhood):\n",
    "            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n",
    "        for i in range(sizes[g]):\n",
    "            for j in range(sizes[g]):\n",
    "                if mt[g][i][j] > 0.0:\n",
    "                    mt[g][i][j] = 1.0\n",
    "    return -1e9 * (1.0 - mt)\n",
    "\n",
    "def planetoid_adj_to_petarv_adj(adj):\n",
    "    size = adj.max() + 1\n",
    "    petarv_adj = scipy.sparse.csr_array((size, size))\n",
    "    for row in adj:\n",
    "        petarv_adj[row[0], row[1]] = 1\n",
    "    return petarv_adj\n",
    "\n",
    "for dataset in ['citeseer', 'cora']:\n",
    "    data = torch_geometric.datasets.Planetoid(root=f'../data/{dataset}', name=dataset)\n",
    "    # data.edge_index is a (2, C) matrix, where C is the number of citations in the data.\n",
    "    # Each row indicates the indices of which paper cites which. This needs to be\n",
    "    # converted into a (P, P) matrix, where P is the number of papers in the data, and\n",
    "    # each entry indicates whether there is a citation between the corresponding papers.\n",
    "    conn = torch.tensor(adj_to_bias(planetoid_adj_to_petarv_adj(data.edge_index).todense()[np.newaxis], [data.x.shape[0]], nhood=1), requires_grad=False)\n",
    "    model = GAT(data.num_features, data.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.005, weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[np.newaxis], conn).squeeze()\n",
    "        loss = torch.nn.functional.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(out.argmax(dim=1)[data.train_mask].eq(data.y[data.train_mask]).sum().item() / int(data.train_mask.sum()))\n",
    "\n",
    "    model.eval()\n",
    "    out = model(data.x[np.newaxis]).squeeze()\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item() / int(data.test_mask.sum())\n",
    "    print('\\n\\n*****************************************************************************************************\\n')\n",
    "    print(f'                                         {dataset} ')\n",
    "    print(f'                                         Total Epochs: 200')\n",
    "    print(f'                                         Test Accuracy: {acc:.4f}')\n",
    "    print('\\n*****************************************************************************************************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cells are our 2nd attempt at implementing GAT from scratch. This is based on\n",
    "GATLayerImp2 (https://github.com/gordicaleksa/pytorch-GAT/blob/main/models/definitions/GAT.py#L349)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f8e42",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GATLayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, n_heads, dropout, concat=True):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.lin = torch.nn.Linear(in_dim, n_heads*out_dim)\n",
    "        self.src_proj = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.Tensor(1, n_heads, out_dim)))\n",
    "        self.trg_proj = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.Tensor(1, n_heads, out_dim)))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.leakyRelu = torch.nn.LeakyReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.concat = concat\n",
    "\n",
    "    def forward(self, data, connectivity):\n",
    "        data1 = self.lin(self.dropout(data)).view(-1, self.n_heads, self.out_dim)\n",
    "        data2 = self.dropout(data1)\n",
    "        src_scores = (data2 * self.src_proj).sum(dim=-1, keepdim=True).permute(1, 0, 2)\n",
    "        trg_scores = (data2 * self.trg_proj).sum(dim=-1, keepdim=True).permute(1, 2, 0)\n",
    "        scores = self.softmax(self.leakyRelu(src_scores + trg_scores) + connectivity)\n",
    "        data3 = torch.bmm(scores.float(), data2.permute(1, 0, 2).float()).permute(1, 0, 2)\n",
    "\n",
    "        if self.concat:\n",
    "            data4 = data3.reshape(-1, self.n_heads*self.out_dim)\n",
    "        else:\n",
    "            data4 = data3.mean(dim=1)\n",
    "            \n",
    "        return data4\n",
    "\n",
    "class GATCora(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = GATLayer(n_features, 8, 8, .6, True)\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.layer2 = GATLayer(64, n_classes, 1, .6, False)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, connectivity):\n",
    "        x = self.elu(self.layer1(x, connectivity))\n",
    "        return self.softmax(self.layer2(x, connectivity))\n",
    "\n",
    "for dataset in ['citeseer', 'cora']:\n",
    "    data = torch_geometric.datasets.Planetoid(root=f'../data/{dataset}', name=dataset)\n",
    "    conn = torch.tensor(adj_to_bias(planetoid_adj_to_petarv_adj(data.edge_index).todense()[np.newaxis], [data.x.shape[0]], nhood=1), requires_grad=False).squeeze()\n",
    "    model = GATCora(data.num_features, data.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=.005, weight_decay=5e-4)\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, conn).squeeze()\n",
    "        loss = torch.nn.functional.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(out.argmax(dim=1)[data.train_mask].eq(data.y[data.train_mask]).sum().item() / int(data.train_mask.sum()))\n",
    "\n",
    "    model.eval()\n",
    "    out = model(data.x).squeeze()\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = pred[data.test_mask].eq(data.y[data.test_mask]).sum().item() / int(data.test_mask.sum())\n",
    "    print('\\n\\n*****************************************************************************************************\\n')\n",
    "    print(f'                                         {dataset} ')\n",
    "    print(f'                                         Total Epochs: 200')\n",
    "    print(f'                                         Test Accuracy: {acc:.4f}')\n",
    "    print('\\n*****************************************************************************************************\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93734166",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lastly, we did a couple of ablation studies. Specifically, we ran the Cora, Citeseer and Pubmed datasets through two single-layer GATs and a three-layer GAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42963d13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SingleLayerGAT1(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = torch_geometric.nn.GATConv(heads=8, out_channels=n_classes, in_channels=n_features, dropout=.6)\n",
    "        self.act = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.act(self.conv(x, edge_index))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SingleLayerGAT2(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv = torch_geometric.nn.GATConv(heads=8, out_channels=n_classes, in_channels=n_features, dropout=.6)\n",
    "        self.act1 = torch.nn.ELU()\n",
    "        self.act2 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.act2(self.act1(self.conv(x, edge_index)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "class ThreeLayerGAT(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch_geometric.nn.GATConv(heads=8, out_channels=8, in_channels=n_features, dropout=.6)\n",
    "        self.act1 = torch.nn.ELU()\n",
    "        self.conv2 = torch_geometric.nn.GATConv(heads=8, out_channels=8, in_channels=64, dropout=.6)\n",
    "        self.act2 = torch.nn.ELU()\n",
    "        self.conv3 = torch_geometric.nn.GATConv(heads=1, out_channels=n_classes, in_channels=64, dropout=.6)\n",
    "        self.act3 = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.act3(self.conv3(self.act2(self.conv2(self.act1(self.conv1(x, edge_index)), edge_index)), edge_index))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "b4fbbdc9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The code to train these models is the same as for our other Pytorch Geometric-based implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}